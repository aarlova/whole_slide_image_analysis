{"cells":[{"cell_type":"markdown","id":"2af0e38b-696d-4724-94a6-d8c6cc796580","metadata":{"id":"2af0e38b-696d-4724-94a6-d8c6cc796580"},"source":["## <font size=\"5\"> Object Detection Inference Notebook using IceVision/FastAI </font> ##"]},{"cell_type":"code","execution_count":null,"id":"c3f39469","metadata":{"id":"c3f39469","outputId":"c57b3316-8057-4804-f4c8-8c76fda18546"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[1m\u001b[1mINFO    \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/aarlova/.icevision/mmdetection_configs/mmdetection_configs-2.20.1/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m\n"]}],"source":["import torch\n","from icevision.all import *\n","from icevision.models import *\n","\n","from fastai.basics import *\n","from fastai.data.all import *\n","from fastai.callback import *\n","\n","torch.cuda.set_device(0)"]},{"cell_type":"code","execution_count":null,"id":"3ac2a0ac","metadata":{"id":"3ac2a0ac"},"outputs":[],"source":["checkpoint_path = '/media/14TB/aarlova_ovarian/Obj_det_retrain/training_log/faster_rcnn_resnet101_fpn_2x_ColorNorm_30_10x'"]},{"cell_type":"markdown","id":"b4011c7f-8461-47c9-b6db-e9e58acd543d","metadata":{"id":"b4011c7f-8461-47c9-b6db-e9e58acd543d"},"source":["### <font size=\"5\"> Recreate model from scratch to be able to import just the .pth checkpoint </font> ###"]},{"cell_type":"code","execution_count":null,"id":"d8799030-329e-4696-9e13-c715d6336269","metadata":{"id":"d8799030-329e-4696-9e13-c715d6336269","outputId":"426e39ef-b8ba-4b21-aa64-4f302b3c6eca","colab":{"referenced_widgets":["2cd706a9cdcb4563a949a149d7f0eabb","a92fad3b741d4908a22019a87fe07268","8b3e69093d864dc396a15e2906ccd9d7"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["1143 657 439\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cd706a9cdcb4563a949a149d7f0eabb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2558 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[1m\u001b[1mINFO    \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a92fad3b741d4908a22019a87fe07268","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2558 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b3e69093d864dc396a15e2906ccd9d7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/243518604 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/aarlova/anaconda3/envs/pytorch_lym_clone/lib/python3.9/site-packages/mmdet/core/anchor/builder.py:16: UserWarning: ``build_anchor_generator`` would be deprecated soon, please use ``build_prior_generator`` \n","  warnings.warn(\n","2022-12-29 11:18:13,199 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet101'}\n","2022-12-29 11:18:13,201 - mmcv - INFO - load model from: torchvision://resnet101\n","2022-12-29 11:18:13,202 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet101\n","2022-12-29 11:18:13,358 - mmcv - WARNING - The model and loaded state dict do not match exactly\n","\n","unexpected key in source state_dict: fc.weight, fc.bias\n","\n","2022-12-29 11:18:13,412 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}\n","2022-12-29 11:18:13,454 - mmcv - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}\n","2022-12-29 11:18:13,465 - mmcv - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'layer': 'Linear', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n","2022-12-29 11:18:13,938 - mmcv - INFO - \n","backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,939 - mmcv - INFO - \n","backbone.bn1.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,940 - mmcv - INFO - \n","backbone.bn1.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,940 - mmcv - INFO - \n","backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,941 - mmcv - INFO - \n","backbone.layer1.0.bn1.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,941 - mmcv - INFO - \n","backbone.layer1.0.bn1.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,942 - mmcv - INFO - \n","backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,942 - mmcv - INFO - \n","backbone.layer1.0.bn2.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,943 - mmcv - INFO - \n","backbone.layer1.0.bn2.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,944 - mmcv - INFO - \n","backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,944 - mmcv - INFO - \n","backbone.layer1.0.bn3.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,945 - mmcv - INFO - \n","backbone.layer1.0.bn3.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,945 - mmcv - INFO - \n","backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,946 - mmcv - INFO - \n","backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,946 - mmcv - INFO - \n","backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,947 - mmcv - INFO - \n","backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,947 - mmcv - INFO - \n","backbone.layer1.1.bn1.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,948 - mmcv - INFO - \n","backbone.layer1.1.bn1.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,949 - mmcv - INFO - \n","backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,949 - mmcv - INFO - \n","backbone.layer1.1.bn2.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,950 - mmcv - INFO - \n","backbone.layer1.1.bn2.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,950 - mmcv - INFO - \n","backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,951 - mmcv - INFO - \n","backbone.layer1.1.bn3.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,951 - mmcv - INFO - \n","backbone.layer1.1.bn3.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,952 - mmcv - INFO - \n","backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,953 - mmcv - INFO - \n","backbone.layer1.2.bn1.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,953 - mmcv - INFO - \n","backbone.layer1.2.bn1.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,954 - mmcv - INFO - \n","backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,954 - mmcv - INFO - \n","backbone.layer1.2.bn2.weight - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,955 - mmcv - INFO - \n","backbone.layer1.2.bn2.bias - torch.Size([64]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,955 - mmcv - INFO - \n","backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,956 - mmcv - INFO - \n","backbone.layer1.2.bn3.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,956 - mmcv - INFO - \n","backbone.layer1.2.bn3.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,957 - mmcv - INFO - \n","backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,958 - mmcv - INFO - \n","backbone.layer2.0.bn1.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,958 - mmcv - INFO - \n","backbone.layer2.0.bn1.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,959 - mmcv - INFO - \n","backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,959 - mmcv - INFO - \n","backbone.layer2.0.bn2.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,960 - mmcv - INFO - \n","backbone.layer2.0.bn2.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,960 - mmcv - INFO - \n","backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,961 - mmcv - INFO - \n","backbone.layer2.0.bn3.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,961 - mmcv - INFO - \n","backbone.layer2.0.bn3.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,962 - mmcv - INFO - \n","backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,962 - mmcv - INFO - \n","backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,963 - mmcv - INFO - \n","backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,963 - mmcv - INFO - \n","backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,964 - mmcv - INFO - \n","backbone.layer2.1.bn1.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,964 - mmcv - INFO - \n","backbone.layer2.1.bn1.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,965 - mmcv - INFO - \n","backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,965 - mmcv - INFO - \n","backbone.layer2.1.bn2.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,966 - mmcv - INFO - \n","backbone.layer2.1.bn2.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,966 - mmcv - INFO - \n","backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,967 - mmcv - INFO - \n","backbone.layer2.1.bn3.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,967 - mmcv - INFO - \n","backbone.layer2.1.bn3.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,967 - mmcv - INFO - \n","backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,968 - mmcv - INFO - \n","backbone.layer2.2.bn1.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,968 - mmcv - INFO - \n","backbone.layer2.2.bn1.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,969 - mmcv - INFO - \n","backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,969 - mmcv - INFO - \n","backbone.layer2.2.bn2.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,970 - mmcv - INFO - \n","backbone.layer2.2.bn2.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,970 - mmcv - INFO - \n","backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,971 - mmcv - INFO - \n","backbone.layer2.2.bn3.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,971 - mmcv - INFO - \n","backbone.layer2.2.bn3.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,972 - mmcv - INFO - \n","backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,972 - mmcv - INFO - \n","backbone.layer2.3.bn1.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,973 - mmcv - INFO - \n","backbone.layer2.3.bn1.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,973 - mmcv - INFO - \n","backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,974 - mmcv - INFO - \n","backbone.layer2.3.bn2.weight - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,974 - mmcv - INFO - \n","backbone.layer2.3.bn2.bias - torch.Size([128]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,974 - mmcv - INFO - \n","backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,975 - mmcv - INFO - \n","backbone.layer2.3.bn3.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,975 - mmcv - INFO - \n","backbone.layer2.3.bn3.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,976 - mmcv - INFO - \n","backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,976 - mmcv - INFO - \n","backbone.layer3.0.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,977 - mmcv - INFO - \n","backbone.layer3.0.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,977 - mmcv - INFO - \n","backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,977 - mmcv - INFO - \n","backbone.layer3.0.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,978 - mmcv - INFO - \n","backbone.layer3.0.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,978 - mmcv - INFO - \n","backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,979 - mmcv - INFO - \n","backbone.layer3.0.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,979 - mmcv - INFO - \n","backbone.layer3.0.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,979 - mmcv - INFO - \n","backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,980 - mmcv - INFO - \n","backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,980 - mmcv - INFO - \n","backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,981 - mmcv - INFO - \n","backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,981 - mmcv - INFO - \n","backbone.layer3.1.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,981 - mmcv - INFO - \n","backbone.layer3.1.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,982 - mmcv - INFO - \n","backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,982 - mmcv - INFO - \n","backbone.layer3.1.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,983 - mmcv - INFO - \n","backbone.layer3.1.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,983 - mmcv - INFO - \n","backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,983 - mmcv - INFO - \n","backbone.layer3.1.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,984 - mmcv - INFO - \n","backbone.layer3.1.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,984 - mmcv - INFO - \n","backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,985 - mmcv - INFO - \n","backbone.layer3.2.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,985 - mmcv - INFO - \n","backbone.layer3.2.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,985 - mmcv - INFO - \n","backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,986 - mmcv - INFO - \n","backbone.layer3.2.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,986 - mmcv - INFO - \n","backbone.layer3.2.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,986 - mmcv - INFO - \n","backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,987 - mmcv - INFO - \n","backbone.layer3.2.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,987 - mmcv - INFO - \n","backbone.layer3.2.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,988 - mmcv - INFO - \n","backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,988 - mmcv - INFO - \n","backbone.layer3.3.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,989 - mmcv - INFO - \n","backbone.layer3.3.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,989 - mmcv - INFO - \n","backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,989 - mmcv - INFO - \n","backbone.layer3.3.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,990 - mmcv - INFO - \n","backbone.layer3.3.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,990 - mmcv - INFO - \n","backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,991 - mmcv - INFO - \n","backbone.layer3.3.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,991 - mmcv - INFO - \n","backbone.layer3.3.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,992 - mmcv - INFO - \n","backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,992 - mmcv - INFO - \n","backbone.layer3.4.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,992 - mmcv - INFO - \n","backbone.layer3.4.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,993 - mmcv - INFO - \n","backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,993 - mmcv - INFO - \n","backbone.layer3.4.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,994 - mmcv - INFO - \n","backbone.layer3.4.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,994 - mmcv - INFO - \n","backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,994 - mmcv - INFO - \n","backbone.layer3.4.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,995 - mmcv - INFO - \n","backbone.layer3.4.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,995 - mmcv - INFO - \n","backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,996 - mmcv - INFO - \n","backbone.layer3.5.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,996 - mmcv - INFO - \n","backbone.layer3.5.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,997 - mmcv - INFO - \n","backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,997 - mmcv - INFO - \n","backbone.layer3.5.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,997 - mmcv - INFO - \n","backbone.layer3.5.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,998 - mmcv - INFO - \n","backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,998 - mmcv - INFO - \n","backbone.layer3.5.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,999 - mmcv - INFO - \n","backbone.layer3.5.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,999 - mmcv - INFO - \n","backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:13,999 - mmcv - INFO - \n","backbone.layer3.6.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,000 - mmcv - INFO - \n","backbone.layer3.6.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,000 - mmcv - INFO - \n","backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,001 - mmcv - INFO - \n","backbone.layer3.6.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,001 - mmcv - INFO - \n","backbone.layer3.6.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,001 - mmcv - INFO - \n","backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,002 - mmcv - INFO - \n","backbone.layer3.6.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,002 - mmcv - INFO - \n","backbone.layer3.6.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,003 - mmcv - INFO - \n","backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,003 - mmcv - INFO - \n","backbone.layer3.7.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,003 - mmcv - INFO - \n","backbone.layer3.7.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,004 - mmcv - INFO - \n","backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,004 - mmcv - INFO - \n","backbone.layer3.7.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,005 - mmcv - INFO - \n","backbone.layer3.7.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,005 - mmcv - INFO - \n","backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,005 - mmcv - INFO - \n","backbone.layer3.7.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,006 - mmcv - INFO - \n","backbone.layer3.7.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,006 - mmcv - INFO - \n","backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,006 - mmcv - INFO - \n","backbone.layer3.8.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,007 - mmcv - INFO - \n","backbone.layer3.8.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,007 - mmcv - INFO - \n","backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,008 - mmcv - INFO - \n","backbone.layer3.8.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,008 - mmcv - INFO - \n","backbone.layer3.8.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,008 - mmcv - INFO - \n","backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,009 - mmcv - INFO - \n","backbone.layer3.8.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,009 - mmcv - INFO - \n","backbone.layer3.8.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,010 - mmcv - INFO - \n","backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,010 - mmcv - INFO - \n","backbone.layer3.9.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,010 - mmcv - INFO - \n","backbone.layer3.9.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,011 - mmcv - INFO - \n","backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,011 - mmcv - INFO - \n","backbone.layer3.9.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,012 - mmcv - INFO - \n","backbone.layer3.9.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,012 - mmcv - INFO - \n","backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,012 - mmcv - INFO - \n","backbone.layer3.9.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,013 - mmcv - INFO - \n","backbone.layer3.9.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,013 - mmcv - INFO - \n","backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,014 - mmcv - INFO - \n","backbone.layer3.10.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,014 - mmcv - INFO - \n","backbone.layer3.10.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,014 - mmcv - INFO - \n","backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,015 - mmcv - INFO - \n","backbone.layer3.10.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,015 - mmcv - INFO - \n","backbone.layer3.10.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,016 - mmcv - INFO - \n","backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,016 - mmcv - INFO - \n","backbone.layer3.10.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,016 - mmcv - INFO - \n","backbone.layer3.10.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,017 - mmcv - INFO - \n","backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,017 - mmcv - INFO - \n","backbone.layer3.11.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,018 - mmcv - INFO - \n","backbone.layer3.11.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,018 - mmcv - INFO - \n","backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,018 - mmcv - INFO - \n","backbone.layer3.11.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,019 - mmcv - INFO - \n","backbone.layer3.11.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,019 - mmcv - INFO - \n","backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,020 - mmcv - INFO - \n","backbone.layer3.11.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,020 - mmcv - INFO - \n","backbone.layer3.11.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,020 - mmcv - INFO - \n","backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,021 - mmcv - INFO - \n","backbone.layer3.12.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,021 - mmcv - INFO - \n","backbone.layer3.12.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,022 - mmcv - INFO - \n","backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,022 - mmcv - INFO - \n","backbone.layer3.12.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,022 - mmcv - INFO - \n","backbone.layer3.12.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,023 - mmcv - INFO - \n","backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,023 - mmcv - INFO - \n","backbone.layer3.12.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,024 - mmcv - INFO - \n","backbone.layer3.12.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,024 - mmcv - INFO - \n","backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,024 - mmcv - INFO - \n","backbone.layer3.13.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,025 - mmcv - INFO - \n","backbone.layer3.13.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,025 - mmcv - INFO - \n","backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,026 - mmcv - INFO - \n","backbone.layer3.13.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,026 - mmcv - INFO - \n","backbone.layer3.13.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,026 - mmcv - INFO - \n","backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,027 - mmcv - INFO - \n","backbone.layer3.13.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,027 - mmcv - INFO - \n","backbone.layer3.13.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,028 - mmcv - INFO - \n","backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,028 - mmcv - INFO - \n","backbone.layer3.14.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,028 - mmcv - INFO - \n","backbone.layer3.14.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,029 - mmcv - INFO - \n","backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,029 - mmcv - INFO - \n","backbone.layer3.14.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,030 - mmcv - INFO - \n","backbone.layer3.14.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,030 - mmcv - INFO - \n","backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,030 - mmcv - INFO - \n","backbone.layer3.14.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,031 - mmcv - INFO - \n","backbone.layer3.14.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,031 - mmcv - INFO - \n","backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,032 - mmcv - INFO - \n","backbone.layer3.15.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,032 - mmcv - INFO - \n","backbone.layer3.15.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,032 - mmcv - INFO - \n","backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,033 - mmcv - INFO - \n","backbone.layer3.15.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,033 - mmcv - INFO - \n","backbone.layer3.15.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,034 - mmcv - INFO - \n","backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,034 - mmcv - INFO - \n","backbone.layer3.15.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,034 - mmcv - INFO - \n","backbone.layer3.15.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,035 - mmcv - INFO - \n","backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,035 - mmcv - INFO - \n","backbone.layer3.16.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,036 - mmcv - INFO - \n","backbone.layer3.16.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,037 - mmcv - INFO - \n","backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,037 - mmcv - INFO - \n","backbone.layer3.16.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,037 - mmcv - INFO - \n","backbone.layer3.16.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,038 - mmcv - INFO - \n","backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,038 - mmcv - INFO - \n","backbone.layer3.16.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,038 - mmcv - INFO - \n","backbone.layer3.16.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,039 - mmcv - INFO - \n","backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,039 - mmcv - INFO - \n","backbone.layer3.17.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,040 - mmcv - INFO - \n","backbone.layer3.17.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,040 - mmcv - INFO - \n","backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,040 - mmcv - INFO - \n","backbone.layer3.17.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,041 - mmcv - INFO - \n","backbone.layer3.17.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,041 - mmcv - INFO - \n","backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,042 - mmcv - INFO - \n","backbone.layer3.17.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,042 - mmcv - INFO - \n","backbone.layer3.17.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,043 - mmcv - INFO - \n","backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,043 - mmcv - INFO - \n","backbone.layer3.18.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,043 - mmcv - INFO - \n","backbone.layer3.18.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,044 - mmcv - INFO - \n","backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,044 - mmcv - INFO - \n","backbone.layer3.18.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,045 - mmcv - INFO - \n","backbone.layer3.18.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,045 - mmcv - INFO - \n","backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,046 - mmcv - INFO - \n","backbone.layer3.18.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,046 - mmcv - INFO - \n","backbone.layer3.18.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,046 - mmcv - INFO - \n","backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,047 - mmcv - INFO - \n","backbone.layer3.19.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,047 - mmcv - INFO - \n","backbone.layer3.19.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,048 - mmcv - INFO - \n","backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,048 - mmcv - INFO - \n","backbone.layer3.19.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,048 - mmcv - INFO - \n","backbone.layer3.19.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,049 - mmcv - INFO - \n","backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,049 - mmcv - INFO - \n","backbone.layer3.19.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,050 - mmcv - INFO - \n","backbone.layer3.19.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,050 - mmcv - INFO - \n","backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,050 - mmcv - INFO - \n","backbone.layer3.20.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,051 - mmcv - INFO - \n","backbone.layer3.20.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,051 - mmcv - INFO - \n","backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,052 - mmcv - INFO - \n","backbone.layer3.20.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,052 - mmcv - INFO - \n","backbone.layer3.20.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,052 - mmcv - INFO - \n","backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,053 - mmcv - INFO - \n","backbone.layer3.20.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,053 - mmcv - INFO - \n","backbone.layer3.20.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,054 - mmcv - INFO - \n","backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,054 - mmcv - INFO - \n","backbone.layer3.21.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,054 - mmcv - INFO - \n","backbone.layer3.21.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,055 - mmcv - INFO - \n","backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,055 - mmcv - INFO - \n","backbone.layer3.21.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,056 - mmcv - INFO - \n","backbone.layer3.21.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,056 - mmcv - INFO - \n","backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,056 - mmcv - INFO - \n","backbone.layer3.21.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,057 - mmcv - INFO - \n","backbone.layer3.21.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,057 - mmcv - INFO - \n","backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,058 - mmcv - INFO - \n","backbone.layer3.22.bn1.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,058 - mmcv - INFO - \n","backbone.layer3.22.bn1.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,058 - mmcv - INFO - \n","backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,059 - mmcv - INFO - \n","backbone.layer3.22.bn2.weight - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,059 - mmcv - INFO - \n","backbone.layer3.22.bn2.bias - torch.Size([256]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,060 - mmcv - INFO - \n","backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,060 - mmcv - INFO - \n","backbone.layer3.22.bn3.weight - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,060 - mmcv - INFO - \n","backbone.layer3.22.bn3.bias - torch.Size([1024]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,061 - mmcv - INFO - \n","backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,061 - mmcv - INFO - \n","backbone.layer4.0.bn1.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,062 - mmcv - INFO - \n","backbone.layer4.0.bn1.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,062 - mmcv - INFO - \n","backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,062 - mmcv - INFO - \n","backbone.layer4.0.bn2.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,063 - mmcv - INFO - \n","backbone.layer4.0.bn2.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,063 - mmcv - INFO - \n","backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,064 - mmcv - INFO - \n","backbone.layer4.0.bn3.weight - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,064 - mmcv - INFO - \n","backbone.layer4.0.bn3.bias - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,065 - mmcv - INFO - \n","backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,065 - mmcv - INFO - \n","backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,065 - mmcv - INFO - \n","backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,066 - mmcv - INFO - \n","backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,066 - mmcv - INFO - \n","backbone.layer4.1.bn1.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,067 - mmcv - INFO - \n","backbone.layer4.1.bn1.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,067 - mmcv - INFO - \n","backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,067 - mmcv - INFO - \n","backbone.layer4.1.bn2.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,068 - mmcv - INFO - \n","backbone.layer4.1.bn2.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,068 - mmcv - INFO - \n","backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,069 - mmcv - INFO - \n","backbone.layer4.1.bn3.weight - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,069 - mmcv - INFO - \n","backbone.layer4.1.bn3.bias - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,070 - mmcv - INFO - \n","backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,070 - mmcv - INFO - \n","backbone.layer4.2.bn1.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,070 - mmcv - INFO - \n","backbone.layer4.2.bn1.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,071 - mmcv - INFO - \n","backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,071 - mmcv - INFO - \n","backbone.layer4.2.bn2.weight - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,072 - mmcv - INFO - \n","backbone.layer4.2.bn2.bias - torch.Size([512]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,072 - mmcv - INFO - \n","backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,072 - mmcv - INFO - \n","backbone.layer4.2.bn3.weight - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,073 - mmcv - INFO - \n","backbone.layer4.2.bn3.bias - torch.Size([2048]): \n","PretrainedInit: load from torchvision://resnet101 \n"," \n","2022-12-29 11:18:14,073 - mmcv - INFO - \n","neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,074 - mmcv - INFO - \n","neck.lateral_convs.0.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,074 - mmcv - INFO - \n","neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,075 - mmcv - INFO - \n","neck.lateral_convs.1.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,075 - mmcv - INFO - \n","neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,075 - mmcv - INFO - \n","neck.lateral_convs.2.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,076 - mmcv - INFO - \n","neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,076 - mmcv - INFO - \n","neck.lateral_convs.3.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,077 - mmcv - INFO - \n","neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,077 - mmcv - INFO - \n","neck.fpn_convs.0.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,077 - mmcv - INFO - \n","neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,078 - mmcv - INFO - \n","neck.fpn_convs.1.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,078 - mmcv - INFO - \n","neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,079 - mmcv - INFO - \n","neck.fpn_convs.2.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,079 - mmcv - INFO - \n","neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n","XavierInit: gain=1, distribution=uniform, bias=0 \n"," \n","2022-12-29 11:18:14,079 - mmcv - INFO - \n","neck.fpn_convs.3.conv.bias - torch.Size([256]): \n","The value is the same before and after calling `init_weights` of FasterRCNN  \n"," \n","2022-12-29 11:18:14,080 - mmcv - INFO - \n","rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,080 - mmcv - INFO - \n","rpn_head.rpn_conv.bias - torch.Size([256]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,081 - mmcv - INFO - \n","rpn_head.rpn_cls.weight - torch.Size([3, 256, 1, 1]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,081 - mmcv - INFO - \n","rpn_head.rpn_cls.bias - torch.Size([3]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,081 - mmcv - INFO - \n","rpn_head.rpn_reg.weight - torch.Size([12, 256, 1, 1]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,082 - mmcv - INFO - \n","rpn_head.rpn_reg.bias - torch.Size([12]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,082 - mmcv - INFO - \n","roi_head.bbox_head.fc_cls.weight - torch.Size([2, 1024]): \n","XavierInit: gain=1, distribution=normal, bias=0 \n"," \n","2022-12-29 11:18:14,083 - mmcv - INFO - \n","roi_head.bbox_head.fc_cls.bias - torch.Size([2]): \n","NormalInit: mean=0, std=0.01, bias=0 \n"," \n","2022-12-29 11:18:14,083 - mmcv - INFO - \n","roi_head.bbox_head.fc_reg.weight - torch.Size([4, 1024]): \n","XavierInit: gain=1, distribution=normal, bias=0 \n"," \n","2022-12-29 11:18:14,083 - mmcv - INFO - \n","roi_head.bbox_head.fc_reg.bias - torch.Size([4]): \n","NormalInit: mean=0, std=0.001, bias=0 \n"," \n","2022-12-29 11:18:14,084 - mmcv - INFO - \n","roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): \n","XavierInit: gain=1, distribution=normal, bias=0 \n"," \n","2022-12-29 11:18:14,084 - mmcv - INFO - \n","roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): \n","XavierInit: gain=1, distribution=normal, bias=0 \n"," \n","2022-12-29 11:18:14,085 - mmcv - INFO - \n","roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): \n","XavierInit: gain=1, distribution=normal, bias=0 \n"," \n","2022-12-29 11:18:14,085 - mmcv - INFO - \n","roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): \n","XavierInit: gain=1, distribution=normal, bias=0 \n"," \n"]},{"name":"stdout","output_type":"stream","text":["load checkpoint from local path: checkpoints/faster_rcnn/faster_rcnn_r101_fpn_2x_coco_bbox_mAP-0.398_20200504_210455-1d2dac9c.pth\n","The model and loaded state dict do not match exactly\n","\n","size mismatch for roi_head.bbox_head.fc_cls.weight: copying a param with shape torch.Size([81, 1024]) from checkpoint, the shape in current model is torch.Size([2, 1024]).\n","size mismatch for roi_head.bbox_head.fc_cls.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([2]).\n","size mismatch for roi_head.bbox_head.fc_reg.weight: copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape in current model is torch.Size([4, 1024]).\n","size mismatch for roi_head.bbox_head.fc_reg.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([4]).\n","faster_rcnn_resnet101_fpn_2x_ColorNorm_30_10x\n"]}],"source":["#### new code\n","data_dir = Path('/media/14TB/aarlova_ovarian/YOLO_data/10x_withEmpty/data')\n","train = [(line.split(\"\\n\",1)[0]) for line in open('/media/14TB/aarlova_ovarian/YOLO_data/10x_withEmpty/train.txt')]\n","val = [(line.split(\"\\n \",1)[0]) for line in open('/media/14TB/aarlova_ovarian/YOLO_data/10x_withEmpty/val.txt')]\n","train = [str.strip('\\n') for str in train]\n","val = [str.strip('\\n') for str in val]\n","\n","test =[(line.split(\"\\n\",1)[0]) for line in open('/media/14TB/aarlova_ovarian/YOLO_data/10x_withEmpty/test.txt')]\n","test = [str.strip('\\n') for str in test]\n","\n","\n","print(len(train), len(val), len(test))\n","\n","data_splitter = FixedSplitter([train,val,test])\n","\n","class_map = ClassMap(['Follicle'])\n","\n","parser = parsers.VOCBBoxParser(annotations_dir=data_dir,\n","                     images_dir=data_dir,\n","                     class_map=class_map)\n","\n","train_records, valid_records, test_records = parser.parse(data_splitter)\n","\n","presize = 512\n","size = 512\n","train_tfms = tfms.A.Adapter([tfms.A.Resize(size, size), tfms.A.Normalize()])\n","valid_tfms = tfms.A.Adapter([tfms.A.Resize(size, size), tfms.A.Normalize()])\n","\n","train_ds = Dataset(train_records, train_tfms)\n","valid_ds = Dataset(valid_records, valid_tfms)\n","\n","len(train_ds), len(valid_ds)\n","\n","\n","# model_type = models.ultralytics.yolov5\n","# backbone = model_type.backbones.small(pretrained=True)\n","# backbone = model_type.backbones.medium(pretrained=True)\n","# backbone = model_type.backbones.large(pretrained=True)\n","#backbone = model_type.backbones.extra_large(pretrained=True)\n","# model = model_type.model(backbone=backbone, num_classes=parser.class_map.num_classes, img_size=size, device=torch.device(\"cuda\"))\n","\n","# model_type = models.torchvision.retinanet\n","# backbone = model_type.backbones.resnet50_fpn\n","\n","model_type = models.mmdet.faster_rcnn\n","\n","backbone = model_type.backbones.resnet101_fpn_2x\n","# Instantiate the mdoel\n","model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map))\n","\n","\n","train_dl = model_type.train_dl(train_ds, batch_size=16, num_workers=2, shuffle=True)\n","valid_dl = model_type.valid_dl(valid_ds, batch_size=16, num_workers=2, shuffle=False)\n","\n","metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]\n","\n","numeph = 30\n","magn = '10x'\n","\n","\n","model_name = 'faster_rcnn' + '_' + 'resnet101_fpn_2x' + '_' + 'ColorNorm_' + str(numeph) + '_' + magn\n","print(model_name)\n","csv_dir = '/media/14TB/aarlova_ovarian/Obj_det_retrain/'\n","\n","\n","# model_name = 'yolov5_small'\n","\n","learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"9d7cca46-3eb6-4b9d-aebe-643f47a52b9f","metadata":{"id":"9d7cca46-3eb6-4b9d-aebe-643f47a52b9f","outputId":"f559114e-7c4f-45a6-ba64-f7817b0ca169"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/aarlova/anaconda3/envs/pytorch_lym_clone/lib/python3.9/site-packages/fastai/learner.py:56: UserWarning: Saved filed doesn't contain an optimizer state.\n","  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"]},{"data":{"text/plain":["<fastai.learner.Learner at 0x7f138428b7c0>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["learn.load(checkpoint_path)"]},{"cell_type":"markdown","id":"1c0b68f5-6fc6-48db-8109-528aab15ff7e","metadata":{"id":"1c0b68f5-6fc6-48db-8109-528aab15ff7e"},"source":["### <font size=\"5\"> Set up test dataset and dataloader, run inference on etire test set (tile-wise) (optional) </font> ###"]},{"cell_type":"code","execution_count":null,"id":"275b5f4f","metadata":{"id":"275b5f4f"},"outputs":[],"source":["infer_ds = Dataset(test_records, valid_tfms) # valid_tfms are appropriate for test dataset in this case\n","infer_dl = model_type.infer_dl(infer_ds, batch_size=4, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"162a4131","metadata":{"id":"162a4131","outputId":"5815f3b0-8ab8-4974-ede2-f3fca89b091a","colab":{"referenced_widgets":["07a869e1fde549ad98f12d3dd57e8a4d"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07a869e1fde549ad98f12d3dd57e8a4d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/110 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/aarlova/anaconda3/envs/pytorch_lym_clone/lib/python3.9/site-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n","  warnings.warn('``grid_anchors`` would be deprecated soon. '\n","/home/aarlova/anaconda3/envs/pytorch_lym_clone/lib/python3.9/site-packages/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n","  warnings.warn(\n"]}],"source":["## run inference on test dataset (optional)\n","preds = model_type.predict_from_dl(model, infer_dl, keep_images=True)"]},{"cell_type":"code","execution_count":null,"id":"d3a6faa9","metadata":{"id":"d3a6faa9"},"outputs":[],"source":["plt.rcParams['figure.dpi'] = 100 # for high resolution figure\n","show_preds(preds=preds[:10])"]},{"cell_type":"markdown","id":"6c4f5c0b-3964-415d-b30f-f71c88bcbd0e","metadata":{"id":"6c4f5c0b-3964-415d-b30f-f71c88bcbd0e"},"source":["### <font size=\"5\">Inference with OpenSlide on Ovarian valid/train slides (one annotated section per slide)</font>\n","<font size=\"3\"> Ground Truth Margin annotations are imported from .json file </font>"]},{"cell_type":"code","execution_count":null,"id":"17b1a9ec-11f3-49d6-8fa0-b9cf8253840e","metadata":{"id":"17b1a9ec-11f3-49d6-8fa0-b9cf8253840e"},"outputs":[],"source":["import openslide\n","import cv2\n","from skimage import morphology\n","import cv2 as cv\n","from shapely.geometry import Polygon, MultiPolygon, box\n","from shapely.ops import unary_union\n","import matplotlib.pyplot as plt\n","import random\n","import numpy as np\n","from shapely.strtree import STRtree\n","from shapely.geometry import mapping\n","\n","import geojson"]},{"cell_type":"code","execution_count":null,"id":"21684b03-2a9f-43cb-8228-2494fae848b5","metadata":{"id":"21684b03-2a9f-43cb-8228-2494fae848b5"},"outputs":[],"source":["def get_grid(polys, tile_size, ol, plot = False):\n","    slide_patches = []\n","    for i in range(len(polys)):\n","        margin_poly = Polygon(polys[i])\n","        minx, miny, maxx, maxy = margin_poly.bounds\n","        minx, miny, maxx, maxy = int(minx), int(miny), int(maxx), int(maxy)\n","\n","        n_cols = int(np.ceil((maxx - minx) / (tile_size - ol)))\n","        n_rows = int(np.ceil((maxy - miny) / (tile_size - ol)))\n","\n","        # all x's\n","        x_zero = int(margin_poly.bounds[0])\n","        xs = []\n","        xs.append(x_zero)\n","        for x in range(n_cols - 1):\n","            next_x = x_zero + (tile_size - ol)\n","            xs.append(next_x)\n","            x_zero = next_x\n","\n","        # all y's\n","        y_zero = int(margin_poly.bounds[1])\n","        ys = []\n","        ys.append(y_zero)\n","        for y in range(n_rows - 1):\n","            next_y = y_zero + (tile_size - ol)\n","            ys.append(next_y)\n","            y_zero = next_y\n","\n","        # assemble into rectangles\n","        patches = []\n","        for w in range(n_cols):\n","            x = xs[w]\n","            for z in range(n_rows):\n","                y = ys[z]\n","                rect = box(x, y, x + tile_size, y + tile_size, ccw=False)\n","                patches.append(rect)\n","\n","        # find patches that intersect with tissue margin\n","        tree = STRtree(patches)\n","        query_geom = margin_poly\n","        intersects = [n for n in tree.query(query_geom) if n.intersects(query_geom) and n.intersection(query_geom).area/n.area > 0.05]\n","\n","        print('number of patches in this section: ', len(intersects))\n","\n","        # append intersecting patches to slide_patches list\n","        slide_patches.append(intersects)\n","\n","        if plot == True:\n","            # let's plot the intersecting ones\n","            fig, ax = plt.subplots()\n","\n","            # plot main_poly\n","            ab = margin_poly.exterior.xy\n","            a, b = np.array(ab[0]), np.array(ab[1])\n","            ax.plot(a, b, color='b', linestyle='-', linewidth=0.4)\n","            ax.invert_yaxis()\n","\n","            # plot other_polies\n","            patches = intersects\n","            color = ['r', 'g', 'b', 'c', 'y', 'm']\n","            for j in range(len(patches)):\n","                x, y = np.array(patches[j].exterior.xy[0]), np.array(patches[j].exterior.xy[1])\n","                plt.plot(x, y, color=color[random.randint(0, 5)], linestyle='-', linewidth=0.4)\n","            ax.grid(linestyle='--', linewidth='0.5')\n","            plt.title(\"Finding Intersecting Polys\")\n","            plt.xlabel(\"X\")\n","            plt.ylabel(\"Y\")\n","            plt.axis('equal')\n","            plt.show()\n","            plt.cla()\n","            plt.clf()\n","            plt.close()\n","\n","    return slide_patches"]},{"cell_type":"code","execution_count":null,"id":"96e263c6-1fe0-444f-851b-f3de387ab2a3","metadata":{"id":"96e263c6-1fe0-444f-851b-f3de387ab2a3"},"outputs":[],"source":["def read_qupath_json(json_path):\n","    with open(json_path) as f:\n","        slide_objects = geojson.load(f)\n","        # slide_areas = [shape(obj[\"geometry\"]).area for obj in slide_objects]\n","        slide_labels = [obj['properties']['classification']['name'] for obj in slide_objects]\n","        slide_object_coordinates = [obj['geometry']['coordinates'][0] for obj in slide_objects]\n","        return slide_object_coordinates, slide_labels"]},{"cell_type":"code","execution_count":null,"id":"e66e87ea-856b-4284-bf19-7fb2bc79d016","metadata":{"id":"e66e87ea-856b-4284-bf19-7fb2bc79d016"},"outputs":[],"source":["def write_qupath_noIDs_Polys(xmlsave, regions, region_labels, region_colors):\n","    dumped = []\n","    trythis = '['\n","    for i in range(0, len(regions)):\n","        # if len(regions[i]) >2:\n","            # roi = Polygon(regions[i])\n","        roi = regions[i]\n","        label = region_labels[i]\n","        trythis += json.dumps(\n","            {\"type\": \"Feature\", \"id\": \"PathAnnotationObject\", \"geometry\": mapping(roi),\n","             \"properties\": {\"classification\": {\"name\": label, \"colorRGB\": region_colors}, \"isLocked\": False,\n","                            \"measurements\": []}}, indent=4)\n","        if i < len(regions) - 1:\n","            trythis += ','\n","        else:\n","            dumped.append([region_labels[i], regions[i]])\n","    trythis += ']'\n","\n","    with open(xmlsave, 'w') as outfile:\n","        outfile.write(trythis)\n","\n","        dumped = []"]},{"cell_type":"code","execution_count":null,"id":"bdea3543","metadata":{"id":"bdea3543"},"outputs":[],"source":["from pathlib import Path\n","\n","# slide_list = ['17118416']#, '17118483', '17118873', '17118922', '17119598', '17122090', '17122178', '17122189', '17122204', '17122207', '17122313', '17124767', '17126247', '17126281', '17126430', '17126465', '17126513', '17133040', '17133223', '17133268', '17133434', '17133461', '17133587', '17133626', '17133630', '17133678', '17133780', '17133781', '17133792', '17133937', '17133968', '17134444', '17134545', '17134750']\n","# slide_list = ['17133945','17122241','17121975', '17133388', '17133503', '17133754', '17134585'] # val\n","# slide_list = ['17121963', '17124810', '17122321', '17133557']#, '17133893'] # test\n","slide_list = ['17133893']\n","region_colors = -16274801\n","json_dir = Path('/media/14TB/aarlova_ovarian/ovarian demo/demo_objDet_preds')"]},{"cell_type":"code","execution_count":null,"id":"14a7c77c-ea18-4216-9a3e-b9a1eb7968cd","metadata":{"id":"14a7c77c-ea18-4216-9a3e-b9a1eb7968cd"},"outputs":[],"source":["requested_magn = 10\n","downsample = int(40/requested_magn)\n","tile_size = int(500*downsample)\n","overlap = int(250*downsample)"]},{"cell_type":"code","execution_count":null,"id":"c2d3c073-4368-4467-a788-d92a63dd0187","metadata":{"id":"c2d3c073-4368-4467-a788-d92a63dd0187","outputId":"3d5ab93d-eb7b-42db-a11c-0d77ecb3fb00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Working on Slide 17133893.svs\n","opened slide!\n","There are  3 annotated margins\n","Set tile size to 2000 , and overlap to 1000\n","number of patches in this section:  65\n","number of patches in this section:  76\n","number of patches in this section:  248\n","Done tiling the slide!\n","Number of sections in slide: 3\n","Number of tiles in current section: 65\n","done processing tiles in section 0\n","Number of tiles in current section: 76\n","done processing tiles in section 1\n","Number of tiles in current section: 248\n","done processing tiles in section 2\n","done with all sections\n","After filtered by aspect ratio 959\n","After merging intersecting polygons 959\n","Len of unique merged polygons after removing duplicates 639\n","Len of unique merged after removing polygons that are covered by others 481\n"]}],"source":["for i in range(len(slide_list)):\n","    slide_fname = Path('/media/14TB/aarlova_ovarian/slides/' + slide_list[i] + '.svs')\n","    print('Working on Slide', slide_fname.name)\n","\n","    slide = openslide.OpenSlide(str(slide_fname))\n","    print('opened slide!')\n","\n","    # get regions within which to perform inference\n","    jsons_fname = Path('/media/14TB/aarlova_ovarian/GT_json_annotations/' + slide_list[i] + '.json')\n","\n","    coords, labels = read_qupath_json(jsons_fname)\n","\n","    tissue_regions = [Polygon(coords[i]) for i in range(len(coords)) if labels[i] == 'Margin']\n","    print('There are ',len(tissue_regions),'annotated margins')\n","\n","    # this is to fix one invalid area - delete this line\n","    tissue_regions[1] = tissue_regions[1].buffer(0)\n","\n","    #######################################################################\n","    # generate a grid of tiles of specific size and overlap within bounds of the Ovary\n","    print('Set tile size to', tile_size, ', and overlap to', overlap)\n","\n","    slide_grid = get_grid(tissue_regions, tile_size, overlap, plot=False) # slide_grid is a nested list of Polygons. Len(slide_grid) is how many sections of tissue are on slide.\n","    print('Done tiling the slide!')\n","\n","    #######################################################################\n","    # tile the slide with my script, get slide_grid\n","    print('Number of sections in slide:', len(slide_grid))\n","\n","    slide_boxes = []\n","\n","    for section in range(len(slide_grid)):\n","        print('Number of tiles in current section:', len(slide_grid[section]))\n","\n","\n","        for tile in range(len(slide_grid[section])):\n","            current_tile = slide_grid[section][tile]\n","            current_tile_top_left_coord = [int(current_tile.bounds[0]), int(current_tile.bounds[1])]  # this should be minx, miny if polygons were created with ccw=False\n","            # tile_name = str(out_tile_dir/slide_fname.stem) + '/' + str(slide_fname.stem) + ' [x=' + str(current_tile_top_left_coord[0]) + ',y=' + str(current_tile_top_left_coord[1]) + ',w='+str(tile_size) + ',h=' + str(tile_size) + '].jpg'\n","            tile_img = slide.read_region((current_tile_top_left_coord[0], current_tile_top_left_coord[1]),0,(tile_size,tile_size)).convert('RGB')\n","            resized_tile = tile_img.resize((500,500))\n","\n","\n","            #2. Predict on one individual tile at a time\n","            pred_dict  = model_type.end2end_detect(resized_tile, valid_tfms, model, class_map=class_map, detection_threshold=0.25)\n","            # pred_dict['img'].show()\n","\n","            poly_boxes = []\n","\n","            a = pred_dict['detection']['bboxes']\n","            if len(a) > 0:\n","                for b in a:\n","                    poly = box(b.xmin, b.ymin, b.xmax, b.ymax)\n","                    poly_boxes.append(poly)\n","\n","\n","\n","            #3. Translate tile-relative coords of Polygon to WSI-relative coords\n","            for p in range(len(poly_boxes)):\n","                new_ext_x = np.array(poly_boxes[p].exterior.coords.xy[0]) * downsample + current_tile_top_left_coord[0]\n","                new_ext_y = np.array(poly_boxes[p].exterior.coords.xy[1]) * downsample + current_tile_top_left_coord[1]\n","\n","                new_coords = list(zip(new_ext_x, new_ext_y))\n","                new_poly = Polygon(new_coords)\n","                slide_boxes.append(new_poly)\n","\n","\n","            tile_img.close()\n","            resized_tile.close()\n","\n","\n","\n","        print('done processing tiles in section', section)\n","\n","\n","\n","    print('done with all sections')\n","\n","    slide.close()\n","\n","    ########### filter out 'too' rectangular polygons\n","    filtered = []\n","    for o in slide_boxes:\n","        x = o.bounds[2] - o.bounds[0]\n","        y = o.bounds[3] - o.bounds[1]\n","\n","        abs_log_aspect_ratio_sample = abs(math.log(x/y))\n","\n","        if abs_log_aspect_ratio_sample <= 0.5:\n","            filtered.append(o)\n","\n","    ########## remove intersecting whose intersection with other is greater than 50% of its area?\n","    tree = STRtree(filtered)\n","\n","    print('After filtered by aspect ratio', len(filtered))\n","    iou = []\n","    all_merged = []\n","    for i in filtered:\n","        query_geom = i\n","        # intersects = [o for o in tree.query(query_geom)\n","        #               if o.intersection(query_geom).area/unary_union([o,query_geom]).area > 0.5]\n","        intersects = [o for o in tree.query(query_geom) if o.intersection(query_geom).area/o.area > 0.5]\n","\n","        merged = unary_union(intersects)\n","        all_merged.append(merged)\n","\n","    print('After merging intersecting polygons', len(all_merged))\n","\n","    ############### remove duplicates from all_merged:\n","    all_merged_coords = [f.bounds for f in all_merged] # convert to list of coords in order to find unique polygons\n","    unique_merged = set(all_merged_coords)\n","    print('Len of unique merged polygons after removing duplicates',len(unique_merged))\n","    unique_merged = [box(*u) for u in unique_merged] # back to Polygons\n","\n","    ############### remove polygons that are covered by other polygons\n","    tree = STRtree(unique_merged)\n","    to_remove = []\n","    for i in unique_merged:\n","        query_geom = i\n","        covered_by = [o for o in tree.query(query_geom) if o.covered_by(query_geom) and not o.equals(query_geom)]\n","        to_remove.extend(covered_by) # list of polygons to remove\n","\n","    ############### remove duplicates\n","    all_merged_coords = [f.bounds for f in unique_merged]\n","    to_remove_coords = [f.bounds for f in to_remove]\n","    unique_merged = set(all_merged_coords) - set(to_remove_coords)\n","    print('Len of unique merged after removing polygons that are covered by others',len(unique_merged))\n","\n","    unique_merged = [box(*u) for u in unique_merged] # back to Polygons\n","\n","    ############## write JSON files\n","    slide_follicles = unique_merged\n","    slide_labels = ['Follicle' for f in slide_follicles]\n","\n","    write_qupath_noIDs_Polys(str(json_dir) + '/' + str(slide_fname.stem) + '_fastRCNN.json', slide_follicles, slide_labels, region_colors)\n","    # write all original preds\n","    slide_labels = ['Follicle' for f in slide_boxes]\n","    write_qupath_noIDs_Polys(str(json_dir) + '/' + str(slide_fname.stem) + '_fastRCNN_all.json', slide_boxes, slide_labels, region_colors)\n"]},{"cell_type":"code","execution_count":null,"id":"19ac9115-bd7c-4a8e-b303-1121c1d14aa4","metadata":{"id":"19ac9115-bd7c-4a8e-b303-1121c1d14aa4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"pytorch_lym_clone","language":"python","name":"pytorch_lym_clone"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}